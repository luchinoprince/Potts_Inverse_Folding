{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook just generate streamlines the code of **generating_sequences.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "#\n",
    "\n",
    "sys.path.insert(1, \"./../util/\")\n",
    "sys.path.insert(1, \"./../model/\")\n",
    "from encoded_protein_dataset_new import get_embedding, EncodedProteinDataset_new, EncodedProteinDataset_aux\n",
    "from dynamic_loader import dynamic_collate_fn, dynamic_cluster\n",
    "\n",
    "from pseudolikelihood import get_npll2, get_npll3\n",
    "import torch, torchvision\n",
    "from potts_decoder import PottsDecoder\n",
    "from test_utils import load_model, get_samples_potts, compute_distances, select_sequences\n",
    "from esm_utils import get_samples_esm\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from functools import partial\n",
    "\n",
    "from typing import Sequence, Tuple, List\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "sys.path.insert(1, \"./../../esm/\")\n",
    "import esm.pretrained as pretrained\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "from dynamic_loader import dynamic_collate_fn, dynamic_cluster\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "from esm_utils import load_structure, extract_coords_from_structure, get_atom_coords_residuewise\n",
    "from esm_utils import sample_esm_batch2\n",
    "from esm_utils import align_esm\n",
    "\n",
    "\n",
    "import pyhmmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let us load the desired test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is:2 length data:1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucasilva/Potts_Inverse_Folding/tests/./../util/encoded_protein_dataset_new.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n"
     ]
    }
   ],
   "source": [
    "max_msas = 2\n",
    "msa_dir = \"./../../split2/\"\n",
    "encoding_dir =\"./../../structure_encodings/\"\n",
    "\n",
    "### Now I am doing superfamily, I did not change the name for convenience\n",
    "test_dataset = EncodedProteinDataset_aux(os.path.join(msa_dir, 'test/superfamily'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "\n",
    "batch_structure_size_train = 1\n",
    "batch_structure_size=1\n",
    "perc_subset_test = 1.0    \n",
    "batch_msa_size = 128 \n",
    "q = 21 \n",
    "\n",
    "collate_fn = partial(dynamic_collate_fn, q=q, batch_size=batch_structure_size, batch_msa_size=batch_msa_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "num_workers=1, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then let us select a set of representatives from the selected test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = os.path.join(msa_dir, \"./test/summary.txt\")\n",
    "summary_df = pd.read_csv(summary_path, sep=\"\\t\")\n",
    "ids = [el[-14:-7] for el in test_dataset.msas_paths]\n",
    "# Filter the DataFrame based on admissible values\n",
    "filtered_df = summary_df[summary_df['# domain'].isin(ids)]\n",
    "sf = filtered_df[filtered_df[\"test_type\"]==\"superfamily\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "grouped_dict = {}\n",
    "\n",
    "# Iterate through the strings\n",
    "for s in sf['superfamily']:\n",
    "    # Split the string into parts\n",
    "    parts = s.split('.')\n",
    "    \n",
    "    # Create 'x.y' key\n",
    "    key = f'{parts[0]}.{parts[1]}'\n",
    "    \n",
    "    # If key is not in the dictionary, add it with an empty list as the value\n",
    "    if key not in grouped_dict:\n",
    "        grouped_dict[key] = []\n",
    "    \n",
    "    # Append the 'z' value to the list\n",
    "    grouped_dict[key].append(parts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "grouped_dict_2 = {}\n",
    "\n",
    "for key in grouped_dict.keys():\n",
    "    # Your input sequence\n",
    "    vals = []\n",
    "    sequence = grouped_dict[key]\n",
    "\n",
    "    # Count the occurrences using Counter\n",
    "    counts = Counter(sequence)\n",
    "    # Extract the non-repeating words and their counts\n",
    "    non_repeating = list(counts.values())\n",
    "    words = list(counts.keys())\n",
    "    vals.append(non_repeating)\n",
    "    vals.append(list(counts.keys()))\n",
    "    grouped_dict_2[key] = vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_set1 = []\n",
    "repr_set = []\n",
    "for key in grouped_dict_2.keys():\n",
    "    #key='3.30'\n",
    "    vals = grouped_dict_2[key]\n",
    "    idx = np.argmax(vals[0])\n",
    "    fold = grouped_dict_2[key][1][idx]\n",
    "    fold_hom = key+\".\"+fold\n",
    "    result = next((i for i, s in enumerate(list(sf['superfamily'].values)) if fold_hom in s), None)    \n",
    "    id = sf['# domain'].values[result]\n",
    "    repr_set1.append(id)\n",
    "    result2 = next((i for i, s in enumerate(test_dataset.msas_paths) if id in s), None) \n",
    "    repr_set.append(result2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can proceed to generate the samples from the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at index: 1 out of 2\n",
      "Got results for ardca\n",
      "Got results for esm\n",
      "initializing sampler... 0.323166 sec\n",
      "\n",
      "sampling model with mcmc... 9.69749 sec\n",
      "updating mcmc stats with samples... 0.628784 sec\n",
      "computing sequence energies and correlations... 0.066324 sec\n",
      "writing final sequences... done\n",
      "GOT SAMPLES FROM POTTS\n",
      "I am at index: 2 out of 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     B, _, N \u001b[38;5;241m=\u001b[39m msas\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     54\u001b[0m     pdb_name \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmsas_paths[idx][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m14\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m     samples_ardca \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_ardca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_ardca_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     samples_ardca \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(samples_ardca\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m####### Now that I have the full samples, I want to compute the distance from the native sequence\u001b[39;00m\n",
      "File \u001b[0;32m~/Potts_Inverse_Folding/tests/./../model/potts_decoder.py:305\u001b[0m, in \u001b[0;36mPottsDecoder.sample_ardca_full\u001b[0;34m(self, encodings, padding_mask, device, n_samples)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m aa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(q):\n\u001b[1;32m    304\u001b[0m         second_idx \u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m*\u001b[39mq \u001b[38;5;241m+\u001b[39m samples[:, acc]\n\u001b[0;32m--> 305\u001b[0m         Ham[:, aa] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m couplings[pos\u001b[38;5;241m*\u001b[39mq\u001b[38;5;241m+\u001b[39maa, second_idx]\u001b[38;5;66;03m#.unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m    306\u001b[0m p_pos \u001b[38;5;241m=\u001b[39m softmax(Ham, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m#p_pos = softmax(-Ham, dim=0)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### rept_set will be a set defined to ensure that we are properly representing the test dataset under analysis\n",
    "### We don't want, for instance, to always select members from the same superfamily. \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "res_full_esm = {}\n",
    "res_full_potts = {}\n",
    "res_full_ardca = {}\n",
    "pdb_dir = './../../dompdb/'\n",
    "\n",
    "Ns = torch.zeros(len(repr_set))\n",
    "Ms = torch.zeros(len(repr_set))\n",
    "\n",
    "############################## FOR ESM ######################################\n",
    "alphabet='ACDEFGHIKLMNPQRSTVWY-'\n",
    "default_index = alphabet.index('-')\n",
    "aa_index = defaultdict(lambda: default_index, {alphabet[i]: i for i in range(len(alphabet))})\n",
    "aa_index_inv = dict(map(reversed, aa_index.items()))   \n",
    "#device='cpu'\n",
    "model_esm, alphabet_esm = pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "model_esm.eval();\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "device = 0\n",
    "################################ FOR ARDCA ####################################\n",
    "bk_dir = \"./../../bk_models2/\"\n",
    "fname_par_ardca = 'model_11_07_2023_epoch_' + str(94.0) + '_ardca' + '.pt'\n",
    "model_path_ardca = os.path.join(bk_dir, fname_par_ardca)\n",
    "decoder_ardca = load_model(model_path_ardca, device=device)\n",
    "\n",
    "############################## FOR POTTS ########################################\n",
    "fname_par_potts = 'model_25_06_2023_epoch_' + str(94.0) + '.pt'\n",
    "model_path_potts = os.path.join(bk_dir, fname_par_potts)\n",
    "decoder_potts = load_model(model_path_potts, device=device)\n",
    "counter=0\n",
    "for idx_bk in repr_set:\n",
    "    print(f\"I am at index: {counter+1} out of {len(repr_set)}\")\n",
    "    M,N = test_dataset[idx_bk][0].shape\n",
    "    Ns[counter] = N\n",
    "    Ms[counter] = M\n",
    "    idx = -1\n",
    "    with torch.no_grad():\n",
    "        for inputs_packed in test_loader:\n",
    "            idx+=1\n",
    "            if idx != idx_bk:\n",
    "                continue\n",
    "            for inputs in inputs_packed[1]:\n",
    "                msas, encodings, padding_mask  = [input.to(device, non_blocking=True) for input in inputs]\n",
    "                B, _, N = msas.shape\n",
    "                pdb_name = test_dataset.msas_paths[idx][-14:-7]\n",
    "                samples_ardca = decoder_ardca.sample_ardca_full(encodings, padding_mask, device=0, n_samples=10000)\n",
    "                samples_ardca = torch.tensor(samples_ardca.to('cpu'), dtype=torch.long)\n",
    "            \n",
    "            ####### Now that I have the full samples, I want to compute the distance from the native sequence\n",
    "            distances_ardca = compute_distances(samples_ardca, idx_bk, test_dataset, pdb_name, aa_index, aa_index_inv)\n",
    "            \n",
    "            ####### Subselect from data ##########\n",
    "            min_dist = np.min(distances_ardca)\n",
    "            if min_dist < 0.65:\n",
    "                percs = [0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "            else:\n",
    "                percs = [0.7, 0.75, 0.8, 0.85]\n",
    "            \n",
    "            res_ardca = select_sequences(distances_ardca, samples_ardca, percs)\n",
    "            print(\"Got results for ardca\")\n",
    "            \n",
    "            ####### Now we move to samples for esm \n",
    "            pdb_path = os.path.join(pdb_dir, pdb_name)\n",
    "\n",
    "            structure =  load_structure(pdb_path)\n",
    "            coords, native_seq = extract_coords_from_structure(structure)\n",
    "            native_seq_num = torch.zeros(len(native_seq), dtype=torch.long)\n",
    "            idx_char=0\n",
    "            for char in native_seq:\n",
    "                native_seq_num[idx_char] = aa_index[char]\n",
    "                idx_char+=1\n",
    "                \n",
    "            \n",
    "            res_esm = get_samples_esm(model_esm, coords, idx_bk, test_dataset, native_seq_num, pdb_name, percs, nfill=10, device=device)\n",
    "            print(\"Got results for esm\")\n",
    "            ##### Now we get samples for Potts\n",
    "            couplings_potts, fields_potts = decoder_potts(encodings, padding_mask)\n",
    "            samples_potts = get_samples_potts(couplings_potts, fields_potts, aa_index, aa_index_inv, N, q)\n",
    "            print(\"GOT SAMPLES FROM POTTS\")\n",
    "            distances_potts = compute_distances(samples_potts, idx_bk, test_dataset, pdb_name, aa_index, aa_index_inv)\n",
    "            \n",
    "            min_dist = np.min(distances_potts)\n",
    "            if min_dist < 0.65:\n",
    "                percs = [0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "            else:\n",
    "                percs = [0.7, 0.75, 0.8, 0.85]\n",
    "            \n",
    "            res_potts = select_sequences(distances_potts, samples_potts, percs)\n",
    "            \n",
    "            ######################## SAVE THE RESULTS ##########################\n",
    "            id = test_dataset.msas_paths[idx_bk][-14:-7]\n",
    "            res_full_potts[id] = res_potts\n",
    "            res_full_ardca[id] = res_ardca\n",
    "            res_full_esm[id] = res_esm\n",
    "            counter+=1\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"samples_ardca_superfamily\", mode=\"wb\") as f:\n",
    "    pickle.dump(res_full_ardca, f)\n",
    "    \n",
    "with open(\"samples_potts_superfamily\", mode=\"wb\") as f:\n",
    "    pickle.dump(res_full_potts, f)\n",
    "    \n",
    "with open(\"samples_esm_superfamily\", mode=\"wb\") as f:\n",
    "    pickle.dump(res_full_esm, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
