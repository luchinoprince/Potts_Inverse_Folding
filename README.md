# Potts_Inverse_Folding
Repository reproducing some of the codes of the paper "Uncovering sequence diversity from a known protein structure".

The repository has different folders which allow for the different steps necessary to train or test the model as shown in the paper. We first begin by the Dataset creation. 

## Dataset creation

This section will require roughly $120Gbs$ of memory while the execution time depends on the CPU resources avaiable to [MMseqs2](https://github.com/soedinglab/MMseqs2). We ran it on _intel I9-13900K/KF 5.8 Ghz_, and this step took roughly a day to complete. The codes can be found in the folder *data_creation*. To run the codes one needs to have downloaded both the _Uniref50_ dataset which can be found on the [UniProt website](https://www.uniprot.org/help/downloads) and the [CATH](http://download.cathdb.info/cath/releases/latest-release/non-redundant-data-sets/) 4.2 40% non redundant dataset. Also one needs to download the [MMseqs2](https://github.com/soedinglab/MMseqs2) and [esm](https://github.com/facebookresearch/esm) libraries.

Once this is done can first run the bash file _create_msas.sh_ to create all the necessary MSA, then one can split them into the train test split outlined in the paper running the python code _train_test_split_. Also to use the current dataloader one has to run  the notebook _get_numerical_MSA.ipynb_ to get all the MSA's in numerical format, so that they are ready to use by the model and don't have to be converted at every update step of the training. To get the **esm1f** pretrained encodings to feed to one of our Potts decoders, one need to run the code _get_encodings_. When running these codes the location of the different input and output directories have to be properly specified. 

## Training models

In the training folder one can find two files, to train respectively the standard pairwise potts model and the autoregressive potts model. If the steps detailled in the **Dataset creation** section have been performed properly, these code should run directly(provided one has all the necessary libraries.).

We trained our model on a single **NVIDIA GeForce RTX 3090**, having $24$ Gbs of memory. Training time obviously depend on the choice of the GPU, but also where the data is stored during training. In our case, we could not store all the data on RAM, hence we loaded every batch during training from a local SSD disk. For the hyperparameter selection reported in the manuscript, and for $94.0$ epochs, the training of the standard potts model took roughly $10$ hours, while the training of the autoregressive potts model took about $24$ hours.

## Test

In the folder tests one can find the codes to replicate some of the tests available in the manuscript. To run these tests one will need to have downloaded on their machine the [bmDCA](https://github.com/ranganathanlab/bmdca) library to generate efficiently MCMC samples from the Potts model, and the [deepStabP](https://github.com/CSBiology/deepStabP) repository for the melting temperature prediction experiment.
 - The file **get_correlations.py** generates samples from the three different models(arDCA, Potts and esm) for a user specified test dataset and the computes the correlation of the covariance matrix of the generated samples with the true one coming from the MSA. For this experiment, we filter the structures to those of lenght smaller thant $512$ and that have at least $2000$ samples in the MSA, so that the "true" covariance coming from the MSA suffieciently reliable. This code takes a long time, especially the Potts generation part which can take up to $3-4$ days on a multi-core machine. One can reduce the size of the testing dataset if needed. We suggest setting some of the [bmDCA](https://github.com/ranganathanlab/bmdca) sampling parameters to low values, especially $resampling_max$, to low values; otherwise we could get stuck for a very long time trying to sample from a very badly conditioned Potts model. Find more details in the manuscript.
 - The notebook **generating_many_sequences.ipynb** allows one to generate the sequences at different hamming distances from the truth for the different models. For this, for every selected test dataset, we select one candidate structure from every avaialable superfamily in the dataset. 
 - The notebook **testing_deepstab.ipynb** predicts the melting temperature for a set of sequences alreadcy generated by the user, for example trough the previous code. If a GPU is available, this code should run in roughly $30$ mins. 
 - The notebook **plotting_correlations.ipynb** allows one to replicate the plots of the manuscript, once the correlations have been generated with the code **get_correlations.py**.

 ## Util folder

 As the name suggests, in this folder we have defined all those function which allow for a modular code in all the other folders above reported. 
